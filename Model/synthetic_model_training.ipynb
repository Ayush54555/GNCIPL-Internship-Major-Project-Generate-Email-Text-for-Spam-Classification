{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f0d90ed"
      },
      "source": [
        "# ***Synthetic Model Training with GPT-2***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfeece68"
      },
      "source": [
        "This notebook demonstrates the process of fine-tuning a GPT-2 model for text generation, specifically for creating email content. It covers data loading, preprocessing, model training, and saving the fine-tuned model. The dataset used contains email text labeled as either spam or ham."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Required Libraries**"
      ],
      "metadata": {
        "id": "POmIsB9z_3DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "import torch"
      ],
      "metadata": {
        "id": "Wxe8hDL8sJ2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Load dataset**"
      ],
      "metadata": {
        "id": "TNIGm4krZWYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plNSoZweUzVK"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/CleanedDataset.csv\"\n",
        "df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **2.Inspect dataset**"
      ],
      "metadata": {
        "id": "lIqP2GMqXzDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shape Of The Dataset"
      ],
      "metadata": {
        "id": "V04UVC7bZeOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(df.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "NelO01F7Xxz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has 83448 rows and 2 columns."
      ],
      "metadata": {
        "id": "OqJZu4PQEbNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First 5 rows"
      ],
      "metadata": {
        "id": "i8RhfiKwZkFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "id": "EwwQXEu1X5s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Info"
      ],
      "metadata": {
        "id": "-qw8Y8o_Zm29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "RcYr-PxcX63-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Handle missing 'text' values"
      ],
      "metadata": {
        "id": "bRa3gOqFvzKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "most_frequent_text = df['text'].mode()[0]\n",
        "df['text'].fillna(most_frequent_text, inplace=True)\n",
        "\n",
        "most_frequent_label = df['label'].mode()[0]\n",
        "df['label'].fillna(most_frequent_label, inplace=True)\n",
        "# Remove exact duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "print(\"Dataset shape after handling nulls and duplicates:\", df.shape)\n",
        "print(\"Number of missing text values:\", df['text'].isna().sum())\n",
        "print(\"Number of missing labels:\", df['label'].isna().sum())\n"
      ],
      "metadata": {
        "id": "j4igshNosYK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Separate spam and ham**\n"
      ],
      "metadata": {
        "id": "ZXYkibh9XnpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_df = df[df['label'] == 1]   #  1 = spam\n",
        "ham_df = df[df['label'] == 0]    #  0 = ham\n",
        "\n",
        "print(\"Spam samples:\", len(spam_df))\n",
        "print(\"Ham samples:\", len(ham_df))"
      ],
      "metadata": {
        "id": "rMOw9EsnXUUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Save as text files for training"
      ],
      "metadata": {
        "id": "Xe5QUWXwZ3w9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spam_file = \"/content/drive/MyDrive/spam.txt\"\n",
        "ham_file = \"/content/drive/MyDrive/ham.txt\"\n",
        "\n",
        "spam_df['cleaned_text'].to_csv(spam_file, index=False, header=False)\n",
        "ham_df['cleaned_text'].to_csv(ham_file, index=False, header=False)\n",
        "\n",
        "print(\" Files saved:\")\n",
        "print(f\"Spam -> {spam_file}, {len(spam_df)} samples\")\n",
        "print(f\"Ham  -> {ham_file}, {len(ham_df)} samples\")"
      ],
      "metadata": {
        "id": "14ey5ipzX-SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.GPT-2 fine-tuning**"
      ],
      "metadata": {
        "id": "tuIf4NUNY-JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "63ROfXQbZA6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "id": "eNEiAq2BYRHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Convert to HuggingFace Dataset"
      ],
      "metadata": {
        "id": "4AtH9kxvtMmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = Dataset.from_pandas(df[['text', 'label']])\n"
      ],
      "metadata": {
        "id": "251ij0rmsB3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split train/test"
      ],
      "metadata": {
        "id": "Z26a1wuGuBSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = dataset_split['train']\n",
        "test_dataset = dataset_split['test']\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Test size:\", len(test_dataset))"
      ],
      "metadata": {
        "id": "a-lTF-KwtUeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Load GPT-2 and Tokenizer"
      ],
      "metadata": {
        "id": "WBUCMAPEuSCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 has no pad token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "2IeepBtwuQf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize Dataset"
      ],
      "metadata": {
        "id": "c_gF7kt9uifv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize, batched=True, batch_size=128)\n",
        "tokenized_test  = test_dataset.map(tokenize, batched=True, batch_size=128)"
      ],
      "metadata": {
        "id": "Yyw9weenuc1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Data Collator"
      ],
      "metadata": {
        "id": "y7HIxWZkuw6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # GPT-2 uses causal LM, not masked LM\n",
        ")\n"
      ],
      "metadata": {
        "id": "uTN6RHE0utQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Training Arguments"
      ],
      "metadata": {
        "id": "f5Mzx4nnu4UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gen_model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,                # Can increase for better accuracy\n",
        "    per_device_train_batch_size=8,     # Adjust based on GPU\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    logging_steps=200,\n",
        "    fp16=True,                         # Use mixed precision if GPU supports\n",
        "    eval_strategy=\"no\",\n",
        "    eval_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    gradient_accumulation_steps=2,\n",
        ")"
      ],
      "metadata": {
        "id": "IUhBjeuEu06R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Trainer Setup"
      ],
      "metadata": {
        "id": "hEGNEogEvA7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train.remove_columns([\"label\"]), # Remove the label column\n",
        "    eval_dataset=tokenized_test,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "7DLBVay1u9oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Start Training"
      ],
      "metadata": {
        "id": "T0w7NtZavISj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "fzKcSeuuvEo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Save Fine-tuned Model"
      ],
      "metadata": {
        "id": "So8z5SIJvP3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./finetuned_gpt2_emails\")\n",
        "tokenizer.save_pretrained(\"./finetuned_gpt2_emails\")\n",
        "print(\"Training complete and model saved!\")"
      ],
      "metadata": {
        "id": "floYlEN9vLkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the folder containing model & tokenizer\n",
        "shutil.make_archive(\"finetuned_gpt2_emails\", 'zip', \"./finetuned_gpt2_emails\")\n",
        "\n",
        "# Download the zip file\n",
        "files.download(\"finetuned_gpt2_emails.zip\")\n"
      ],
      "metadata": {
        "id": "q5i083aQPXWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}