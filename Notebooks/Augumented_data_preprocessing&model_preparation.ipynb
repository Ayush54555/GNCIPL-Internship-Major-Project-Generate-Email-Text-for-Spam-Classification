{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  **Augmented Data Preprocessing & Model Preparation**"
      ],
      "metadata": {
        "id": "HqTatAul6HPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objective**\n",
        "The goal of this notebook is to preprocess, clean, and augment the dataset to prepare it for model training.  \n",
        "We aim to create a robust and balanced dataset that enhances model performance and generalization.  \n",
        "This notebook also includes initial research and selection of suitable supervised learning algorithms for the classification task.\n",
        "\n",
        "---\n",
        "\n",
        "## **Introduction**\n",
        "In this notebook, we focus on preparing and enhancing the dataset for our classification model through data preprocessing and augmentation techniques.  \n",
        "The goal is to ensure that the data is clean, balanced, and suitable for training robust machine learning models.  \n",
        "We perform key steps such as data inspection, cleaning, preprocessing, and augmentation before moving into model research and preparation.  \n",
        "This process forms the foundation for building a reliable predictive model that generalizes well on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5vjeNbto6d7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*1.Preprocessing of synthetic data*"
      ],
      "metadata": {
        "id": "IyQj-a79qq8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Import libraries"
      ],
      "metadata": {
        "id": "EcryhQd8pCQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "!pip install plotly ipywidgets wordcloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, widgets\n"
      ],
      "metadata": {
        "id": "smcWdeJjJkAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Load Data"
      ],
      "metadata": {
        "id": "x1NbJnu9L5s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "KsFpbBFZ4rwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "augmented_path = \"/content/drive/MyDrive/augmented_email.csv\"\n",
        "df_augmented = pd.read_csv(augmented_path)\n",
        "\n",
        "print(\"Augmented data shape:\", df_augmented.shape)"
      ],
      "metadata": {
        "id": "CY5Oa4tA5qxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## first 5 rows"
      ],
      "metadata": {
        "id": "X63gcg9jMpVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_augmented.head()"
      ],
      "metadata": {
        "id": "F31AhviSMC_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Info"
      ],
      "metadata": {
        "id": "JqGlys0AMwzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_augmented.info())"
      ],
      "metadata": {
        "id": "GM0itUwaMwIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description"
      ],
      "metadata": {
        "id": "ngofq6sNM36d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_augmented.describe(include='all'))"
      ],
      "metadata": {
        "id": "GqaMnEfxM3RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical summary"
      ],
      "metadata": {
        "id": "wemGFiY2v4ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df_augmented['text_length'] = df_augmented['cleaned_text'].apply(len)\n",
        "df_augmented['word_count'] = df_augmented['cleaned_text'].apply(lambda x: len(x.split()))\n",
        "df_augmented['avg_word_length'] = df_augmented['cleaned_text'].apply(lambda x: np.mean([len(w) for w in x.split()]))\n",
        "\n",
        "df_augmented[['text_length', 'word_count', 'avg_word_length']].describe()"
      ],
      "metadata": {
        "id": "MBn8WucSvs3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check missing values per column"
      ],
      "metadata": {
        "id": "0M4ye4BiK-Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(df_augmented.isnull().sum())\n"
      ],
      "metadata": {
        "id": "2Dr2WVo-KeNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill missing 'cleaned_text' with 'text' column"
      ],
      "metadata": {
        "id": "mKaaIK9NKotE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_augmented['cleaned_text'] = df_augmented['cleaned_text'].fillna(df_augmented['text'])"
      ],
      "metadata": {
        "id": "httpPLMYKmvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop rows where 'cleaned_text' or 'label' is still missing"
      ],
      "metadata": {
        "id": "DNXzFrHaKuXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_augmented = df_augmented.dropna(subset=['cleaned_text', 'label'])"
      ],
      "metadata": {
        "id": "qBhDAXPfKtOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Remove duplicates"
      ],
      "metadata": {
        "id": "VU-j9Ae5K5JO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_augmented = df_augmented.drop_duplicates(subset=['cleaned_text']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "G-oOrMvKK3wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Preprocessing function"
      ],
      "metadata": {
        "id": "huxUYZNCLFFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df_augmented['cleaned_text'] = df_augmented['cleaned_text'].apply(preprocess_text)\n",
        "\n",
        "# Check results\n",
        "print(\"Shape after handling missing, duplicates, and preprocessing:\", df_augmented.shape)\n",
        "print(df_augmented.head())\n"
      ],
      "metadata": {
        "id": "Rkz3Wg9a6bZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize and count"
      ],
      "metadata": {
        "id": "8YHf_Jgfv_3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_top_n_words(texts, n=20):\n",
        "    all_words = \" \".join(texts).split()\n",
        "    return Counter(all_words).most_common(n)\n",
        "\n",
        "print(\"Top 20 words in Spam:\")\n",
        "print(get_top_n_words(df_augmented[df_augmented['label']=='spam']['cleaned_text']))\n",
        "\n",
        "print(\"\\nTop 20 words in Ham:\")\n",
        "print(get_top_n_words(df_augmented[df_augmented['label']=='ham']['cleaned_text']))"
      ],
      "metadata": {
        "id": "ikmtAW2BvPdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.Exploratory data analysis**"
      ],
      "metadata": {
        "id": "uG08_D2ayMYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Label Distribution"
      ],
      "metadata": {
        "id": "0w6dhh6swdQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "ax = sns.countplot(x='label', data=df_augmented, palette='coolwarm', hue='label', legend=False)\n",
        "plt.title(\"ðŸ“Š Label Distribution (Synthetic Data)\", fontsize=14)\n",
        "plt.xlabel(\"Label (0 = Ham, 1 = Spam)\")\n",
        "plt.ylabel(\"Count\")\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + 0.35, p.get_height()+100))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YdWvvUJUu1m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Length Distribution"
      ],
      "metadata": {
        "id": "_bQEmhKiyKQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_augmented['text_length'] = df_augmented['cleaned_text'].apply(len)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(df_augmented[df_augmented['label']=='ham']['text_length'], bins=40, alpha=0.7, label='Ham')\n",
        "plt.hist(df_augmented[df_augmented['label']=='spam']['text_length'], bins=40, alpha=0.7, label='Spam')\n",
        "plt.legend()\n",
        "plt.title(\"Text Length Distribution (Synthetic Data)\")\n",
        "plt.xlabel(\"Length of Email\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kgZBH2aTvBb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KDE (Density) Plots for Text Features"
      ],
      "metadata": {
        "id": "27nv-POZwUG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(data=df_augmented, x='text_length', hue='label', fill=True, palette='coolwarm')\n",
        "plt.title(\"ðŸ“ˆ Text Length Density by Label\", fontsize=14)\n",
        "plt.xlabel(\"Text Length\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(data=df_augmented, x='word_count', hue='label', fill=True, palette='mako')\n",
        "plt.title(\"ðŸ§® Word Count Density by Label\", fontsize=14)\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8DaRrXzWwDam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Heatmap"
      ],
      "metadata": {
        "id": "O3PW7Z_vwQE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert 'label' column to numeric if it's not already\n",
        "df_augmented['label'] = df_augmented['label'].replace({'ham': 0, 'spam': 1, '1': 1, '0': 0}).astype(int)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(df_augmented[['text_length', 'word_count', 'avg_word_length', 'label']].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\" Feature Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ITCp1q3VwMpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Cloud Comparison"
      ],
      "metadata": {
        "id": "r95izKQEwmV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "spam_words = \" \".join(df_augmented[df_augmented['label']==1]['cleaned_text'])\n",
        "ham_words = \" \".join(df_augmented[df_augmented['label']==0]['cleaned_text'])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
        "axes[0].imshow(WordCloud(width=800, height=400, colormap='Reds').generate(spam_words))\n",
        "axes[0].set_title(\"Spam Word Cloud\", fontsize=14)\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "axes[1].imshow(WordCloud(width=800, height=400, colormap='Blues').generate(ham_words))\n",
        "axes[1].set_title(\"Ham Word Cloud\", fontsize=14)\n",
        "axes[1].axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fN-lW7zgwoVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 20 Most Frequent Words (Bar Charts)"
      ],
      "metadata": {
        "id": "wgLRJBxZwy0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def top_words(df, label, n=20):\n",
        "    words = \" \".join(df[df['label']==label]['cleaned_text']).split()\n",
        "    return pd.DataFrame(Counter(words).most_common(n), columns=['word', 'count'])\n",
        "\n",
        "top_spam = top_words(df_augmented, 1)\n",
        "top_ham = top_words(df_augmented, 0)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
        "sns.barplot(y='word', x='count', data=top_spam, palette='Reds_r', ax=axes[0])\n",
        "axes[0].set_title(\"Top 20 Words in Spam\", fontsize=14)\n",
        "sns.barplot(y='word', x='count', data=top_ham, palette='Blues_r', ax=axes[1])\n",
        "axes[1].set_title(\"Top 20 Words in Ham\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DcjrefyMw1IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***2.Augmenting cleaned and synthetic data***"
      ],
      "metadata": {
        "id": "-5-oDRebqz74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Load the original cleaned dataset"
      ],
      "metadata": {
        "id": "18xtaRYBraGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cleaned_path = \"/content/drive/MyDrive/CleanedDataset.csv\"\n",
        "df_cleaned = pd.read_csv(cleaned_path)\n"
      ],
      "metadata": {
        "id": "mpSmN_M96tUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Check original dataset info"
      ],
      "metadata": {
        "id": "rk2culrArds1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Original CleanedDataset shape:\", df_cleaned.shape)\n",
        "print(df_cleaned.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "61fYaXmirCju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Merge datasets"
      ],
      "metadata": {
        "id": "UDvUqCFRriOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_merged = pd.concat([df_cleaned, df_augmented], ignore_index=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "3HMphAqlrEIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Remove duplicates across the merged dataset"
      ],
      "metadata": {
        "id": "Qjg4a_dBrkub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_merged = df_merged.drop_duplicates(subset=['cleaned_text']).reset_index(drop=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZtfCjgnfrFmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Shuffle the merged dataset"
      ],
      "metadata": {
        "id": "XO_BHb_rroln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_merged = df_merged.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "1UKph6bUrHG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Save merged dataset\n",
        "merged_path = \"/content/drive/MyDrive/merged_dataset.csv\"\n",
        "df_merged.to_csv(merged_path, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "P55nWezIrIex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Verify\n",
        "print(\"Merged dataset shape:\", df_merged.shape)\n",
        "print(df_merged.head())"
      ],
      "metadata": {
        "id": "JBbZJDQerJp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Load the merged dataset"
      ],
      "metadata": {
        "id": "5arTpfLLr2ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "merged_path = \"/content/drive/MyDrive/merged_dataset.csv\"\n",
        "df_merged = pd.read_csv(merged_path)"
      ],
      "metadata": {
        "id": "853Z7cfcr0o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Preprocessing function"
      ],
      "metadata": {
        "id": "KS94jho0r6dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()  # lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)  # remove email addresses\n",
        "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
        "    return text"
      ],
      "metadata": {
        "id": "LJEAgls-r8sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Apply preprocessing to the merged dataset"
      ],
      "metadata": {
        "id": "euPj7BliyoTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_merged['cleaned_text'] = df_merged['cleaned_text'].astype(str).apply(preprocess_text)"
      ],
      "metadata": {
        "id": "HlMba324r_AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Remove any empty or duplicate rows after preprocessing"
      ],
      "metadata": {
        "id": "qa7bcUj_yquz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_merged = df_merged[df_merged['cleaned_text'] != '']  # remove empty\n",
        "df_merged = df_merged.drop_duplicates(subset=['cleaned_text']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "c3ULGn-8sAXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop unnecessary column"
      ],
      "metadata": {
        "id": "fNcoMPG-ysnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df_final = df_merged[['label', 'cleaned_text']]\n"
      ],
      "metadata": {
        "id": "s09UiNAf66SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map labels"
      ],
      "metadata": {
        "id": "l8mZ9r9ryvU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df_final['label'] = df_final['label'].replace({\n",
        "    'ham': 0,\n",
        "    'spam': 1,\n",
        "    0: 0,\n",
        "    1: 1\n",
        "}).astype(int)\n"
      ],
      "metadata": {
        "id": "jCcM54At7XHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Normalize labels"
      ],
      "metadata": {
        "id": "F56WaNXezUEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_final['label'] = df_final['label'].replace({'ham': 0, 'spam': 1})\n",
        "df_final['label'] = df_final['label'].astype(int)"
      ],
      "metadata": {
        "id": "CvOAjiJWs1DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Remove tokenization artifacts"
      ],
      "metadata": {
        "id": "dTVKt_OwzWZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_artifacts(text):\n",
        "    text = re.sub(r'\\b(escapenumb|escapelong|escap|esc)\\b', '', str(text))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df_final['cleaned_text'] = df_final['cleaned_text'].apply(clean_artifacts)"
      ],
      "metadata": {
        "id": "0pdjCHR9s2_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dropping duplicates\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "buy5UMtC41_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_final = df_final.drop_duplicates(subset=['cleaned_text']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Z_T0p1Tus5B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Label Distribution:"
      ],
      "metadata": {
        "id": "w2ME7ItW48r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "label_counts = df_final['label'].value_counts()\n",
        "print(\"\\nLabel Distribution:\\n\", label_counts)\n"
      ],
      "metadata": {
        "id": "VvTbLn6B8IHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_top_words(texts, n=20):\n",
        "    words = \" \".join(texts).split()\n",
        "    return Counter(words).most_common(n)\n",
        "\n",
        "ham_top = get_top_words(df_final[df_final['label'] == 0]['cleaned_text'])\n",
        "spam_top = get_top_words(df_final[df_final['label'] == 1]['cleaned_text'])\n",
        "\n",
        "print(\"\\nTop 20 Words in Ham:\\n\", ham_top)\n",
        "print(\"\\nTop 20 Words in Spam:\\n\", spam_top)"
      ],
      "metadata": {
        "id": "yeMRSLQStHhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Text length analysis"
      ],
      "metadata": {
        "id": "w6jXtliEzHIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_final['text_len'] = df_final['cleaned_text'].apply(len)\n",
        "ham_lens = df_final[df_final['label'] == 0]['text_len']\n",
        "spam_lens = df_final[df_final['label'] == 1]['text_len']\n",
        "\n",
        "print(\"Avg Lengths -> Ham:\", ham_lens.mean(), \" Spam:\", spam_lens.mean())"
      ],
      "metadata": {
        "id": "5LeiJ3QOtCLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory data analysis**"
      ],
      "metadata": {
        "id": "73aJfg5Y5Iet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "1QuVKWXAte61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/final_dataset_clean.csv\")\n",
        "\n",
        "# Add text length feature\n",
        "df['text_length'] = df['cleaned_text'].astype(str).apply(len)\n",
        "\n",
        "# Preview\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Zn-d9CdW1Ip9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Label Distribution"
      ],
      "metadata": {
        "id": "Tr6YEmQt2bxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.pie(df, names='label', title='Spam vs Ham Distribution',\n",
        "             color='label', color_discrete_map={0:'lightblue', 1:'salmon'})\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "NUGTCQnF2Qrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Length Distribution"
      ],
      "metadata": {
        "id": "3OkvJdvJ2fzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.histogram(df, x='text_length', color='label', nbins=100,\n",
        "                   barmode='overlay', opacity=0.7,\n",
        "                   labels={'text_length':'Email Length', 'label':'Label'},\n",
        "                   title='Text Length Distribution by Label')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "bWejMetD2Zv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Top Words Comparison (Bar Plot)"
      ],
      "metadata": {
        "id": "hV7ADhoE2rpp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d2d07c2"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_top_n_words(texts, n=20):\n",
        "    all_words = \" \".join(texts).split()\n",
        "    return pd.DataFrame(Counter(all_words).most_common(n), columns=['word', 'count'])\n",
        "\n",
        "top_spam_words = get_top_n_words(df[df['label']==1]['cleaned_text'].astype(str), 20)\n",
        "top_ham_words = get_top_n_words(df[df['label']==0]['cleaned_text'].astype(str), 20)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "sns.barplot(x='count', y='word', data=top_spam_words, ax=axes[0], palette='Reds_r', hue='word', legend=False)\n",
        "axes[0].set_title('Top 20 Words in Spam Emails')\n",
        "axes[0].set_xlabel('Count')\n",
        "axes[0].set_ylabel('Word')\n",
        "\n",
        "sns.barplot(x='count', y='word', data=top_ham_words, ax=axes[1], palette='Blues_r', hue='word', legend=False)\n",
        "axes[1].set_title('Top 20 Words in Ham Emails')\n",
        "axes[1].set_xlabel('Count')\n",
        "axes[1].set_ylabel('Word')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Add More Features"
      ],
      "metadata": {
        "id": "y0IiWcwq3ovp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add word count feature\n",
        "df['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Add average word length\n",
        "df['avg_word_len'] = df['cleaned_text'].apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split())>0 else 0)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "F0V5agZL3oTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scatterplot: Text Length vs Word Count"
      ],
      "metadata": {
        "id": "fzsCyMAk3zBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(df, x='word_count', y='text_length', color='label',\n",
        "                 hover_data=['cleaned_text'], opacity=0.7,\n",
        "                 labels={'word_count':'Word Count', 'text_length':'Text Length', 'label':'Label'},\n",
        "                 title='Text Length vs Word Count by Label')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "jXH8UCWr2odX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scatterplot: Text Length vs Avg Word Length"
      ],
      "metadata": {
        "id": "oBQ62JHR337E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(df, x='avg_word_len', y='text_length', color='label',\n",
        "                 hover_data=['cleaned_text'], opacity=0.7,\n",
        "                 labels={'avg_word_len':'Average Word Length', 'text_length':'Text Length', 'label':'Label'},\n",
        "                 title='Text Length vs Average Word Length by Label')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "t0a0sApl3uJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Correlation Heatmap"
      ],
      "metadata": {
        "id": "mrLxmmJZ3-vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numeric features\n",
        "num_df = df[['text_length','word_count','avg_word_len','label']]\n",
        "\n",
        "fig = px.imshow(num_df.corr(), text_auto=True, color_continuous_scale='RdBu_r',\n",
        "                title='Feature Correlation Heatmap')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "Mj2WPRtm38E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **statistical testing**"
      ],
      "metadata": {
        "id": "4ZeLnsJP0Wgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chi-Square Test for Label Balance"
      ],
      "metadata": {
        "id": "WorgQgek0TPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create contingency table\n",
        "label_counts = clean_df['label'].value_counts()\n",
        "print(\"Label Counts:\\n\", label_counts)\n",
        "\n",
        "# Chi-square test\n",
        "chi2, p, dof, ex = chi2_contingency([[label_counts[0], label_counts[1]]])\n",
        "print(f\"Chi-Square Test: chi2 = {chi2:.4f}, p-value = {p:.4f}\")\n"
      ],
      "metadata": {
        "id": "7j7DQlEA0bQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset has a good balance between spam and ham emails, so, can proceed with modeling without worrying about severe label imbalance affecting the results."
      ],
      "metadata": {
        "id": "RrXlgiLH1doJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KS test"
      ],
      "metadata": {
        "id": "igTK0ieY4jkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# Add text length column\n",
        "clean_df['text_length'] = clean_df['cleaned_text'].astype(str).apply(len)\n",
        "\n",
        "# Separate by label\n",
        "spam_lengths = clean_df[clean_df['label'] == 1]['text_length']\n",
        "ham_lengths = clean_df[clean_df['label'] == 0]['text_length']\n",
        "\n",
        "\n",
        "ks_stat, ks_p = ks_2samp(spam_lengths, ham_lengths)\n",
        "print(f\"KS Test: statistic = {ks_stat:.4f}, p-value = {ks_p:.4f}\")"
      ],
      "metadata": {
        "id": "TB3qk4MR0bh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "insight: Spam and ham emails tend to have different text lengths.\n",
        "\n",
        "This is important for feature engineering â€” text length can be a useful predictive feature for your spam classifier."
      ],
      "metadata": {
        "id": "U6E7Adiz1WkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Frequency Comparison"
      ],
      "metadata": {
        "id": "kD0c19Rb1E4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split words\n",
        "spam_words = ' '.join(clean_df[clean_df['label']==1]['cleaned_text'].astype(str)).split()\n",
        "ham_words = ' '.join(clean_df[clean_df['label']==0]['cleaned_text'].astype(str)).split()\n",
        "\n",
        "# Count top 20 words\n",
        "spam_counter = Counter(spam_words).most_common(20)\n",
        "ham_counter = Counter(ham_words).most_common(20)\n",
        "\n",
        "print(\"Top 20 Spam Words:\", spam_counter)\n",
        "print(\"Top 20 Ham Words:\", ham_counter)"
      ],
      "metadata": {
        "id": "bwKbZS1O0svZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Model Selection (Initial Research)**\n",
        "Based on the Exploratory Data Analysis (EDA) and the overall characteristics of the dataset, several supervised learning algorithms are considered for the classification task.  \n",
        "The shortlisted models for initial experimentation include:\n",
        "\n",
        "- **Naive Bayes** â€“ A probabilistic model suitable for text-based and categorical data.  \n",
        "- **Logistic Regression** â€“ A simple yet effective linear model for binary or multi-class classification.  \n",
        "- **Random Forest** â€“ An ensemble of decision trees that reduces variance and improves stability.  \n",
        "- **XGBoost** â€“ A gradient boosting algorithm known for strong performance on tabular data.  \n",
        "- **Simple Neural Network** â€“ A baseline deep learning model capable of capturing non-linear relationships.\n",
        "\n",
        "These models will be trained and compared to determine which algorithm performs best for our dataset.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "A7wcisis62gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "In this notebook, we have completed essential steps of data cleaning, preprocessing, and augmentation to prepare the dataset for model training.  \n",
        "We also performed an initial model selection study to shortlist supervised learning algorithms suitable for our classification problem.  \n",
        "The next step involves implementing these models, evaluating their performance using metrics like accuracy, precision, recall, and F1-score, and selecting the most robust one for deployment.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CrHPbgMV660v"
      }
    }
  ]
}