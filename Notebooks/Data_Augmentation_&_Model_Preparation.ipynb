{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation & Model Preparation\n",
        "\n",
        "\n",
        "In this phase, our focus is on **enhancing the dataset with synthetic emails** and preparing it for classification using various machine learning models. By generating high-quality synthetic spam and ham emails, we aim to address class imbalance and improve the robustness of our spam detection system.\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. **Synthetic Data Generation**\n",
        "   - Fine-tune a generative AI model (e.g., GPT-2 or T5) on the real email dataset.\n",
        "   - Generate new synthetic emails for both spam and ham categories.\n",
        "   - Preprocess and clean synthetic emails to match the format of real emails.\n",
        "   - Combine real and synthetic emails to create the final **augmented dataset**.\n",
        "\n",
        "2. **Exploratory Data Analysis & Feature Engineering**\n",
        "   - Perform quick overview and detailed EDA to understand data characteristics.\n",
        "   - Apply **TF-IDF vectorization** to convert text data into numerical features suitable for machine learning.\n",
        "\n",
        "3. **Model Selection (Initial Research)**\n",
        "   - Based on EDA and dataset characteristics, shortlist potential supervised learning algorithms:\n",
        "     - Naive Bayes, Logistic Regression, Random Forest, XGBoost\n",
        "     - Simple neural networks (MLP) for deep learning approaches\n",
        "   - Consider unsupervised learning and anomaly detection for detecting rare or new spam patterns.\n",
        "\n",
        "---\n",
        "\n",
        "By the end of this phase, we will have a **cleaned, augmented, and vectorized dataset** ready for training multiple models, along with a clear plan for model selection and evaluation.\n"
      ],
      "metadata": {
        "id": "PmzTPxAs0nvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Load dataset**"
      ],
      "metadata": {
        "id": "TNIGm4krZWYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plNSoZweUzVK"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/CleanedDataset.csv\"\n",
        "df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **2.Inspect dataset**"
      ],
      "metadata": {
        "id": "lIqP2GMqXzDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shape Of The Dataset"
      ],
      "metadata": {
        "id": "V04UVC7bZeOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(df.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "NelO01F7Xxz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has 83448 rows and 2 columns."
      ],
      "metadata": {
        "id": "OqJZu4PQEbNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First 5 rows"
      ],
      "metadata": {
        "id": "i8RhfiKwZkFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "id": "EwwQXEu1X5s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Info"
      ],
      "metadata": {
        "id": "-qw8Y8o_Zm29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "RcYr-PxcX63-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Separate spam and ham**\n"
      ],
      "metadata": {
        "id": "ZXYkibh9XnpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_df = df[df['label'] == 1]   #  1 = spam\n",
        "ham_df = df[df['label'] == 0]    #  0 = ham\n",
        "\n",
        "print(\"Spam samples:\", len(spam_df))\n",
        "print(\"Ham samples:\", len(ham_df))"
      ],
      "metadata": {
        "id": "rMOw9EsnXUUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Save as text files for training"
      ],
      "metadata": {
        "id": "Xe5QUWXwZ3w9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spam_file = \"/content/drive/MyDrive/spam.txt\"\n",
        "ham_file = \"/content/drive/MyDrive/ham.txt\"\n",
        "\n",
        "spam_df['cleaned_text'].to_csv(spam_file, index=False, header=False)\n",
        "ham_df['cleaned_text'].to_csv(ham_file, index=False, header=False)\n",
        "\n",
        "print(\" Files saved:\")\n",
        "print(f\"Spam -> {spam_file}, {len(spam_df)} samples\")\n",
        "print(f\"Ham  -> {ham_file}, {len(ham_df)} samples\")"
      ],
      "metadata": {
        "id": "14ey5ipzX-SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.GPT-2 fine-tuning**"
      ],
      "metadata": {
        "id": "tuIf4NUNY-JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "63ROfXQbZA6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "id": "eNEiAq2BYRHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Subset for Quick Training"
      ],
      "metadata": {
        "id": "828Ig5A0aANI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load spam + ham (already saved earlier)\n",
        "spam_file = \"/content/drive/MyDrive/spam.txt\"\n",
        "ham_file = \"/content/drive/MyDrive/ham.txt\"\n",
        "\n",
        "# Read into pandas\n",
        "spam = pd.read_csv(spam_file, names=[\"text\"])\n",
        "ham = pd.read_csv(ham_file, names=[\"text\"])\n",
        "\n",
        "# Sample smaller subset for training (2K spam + 2K ham)\n",
        "spam_sample = spam.sample(n=2000, random_state=42) if len(spam) > 2000 else spam\n",
        "ham_sample = ham.sample(n=2000, random_state=42) if len(ham) > 2000 else ham\n",
        "\n",
        "# Merge + add label tokens <SPAM>/<HAM>\n",
        "spam_sample[\"text\"] = \"<SPAM> \" + spam_sample[\"text\"].astype(str)\n",
        "ham_sample[\"text\"] = \"<HAM> \" + ham_sample[\"text\"].astype(str)\n",
        "\n",
        "subset = pd.concat([spam_sample, ham_sample])\n",
        "subset.to_csv(\"subset_emails.txt\", index=False, header=False)\n",
        "\n",
        "print(\" Training subset ready with\", len(subset), \"emails\")\n"
      ],
      "metadata": {
        "id": "ouFkg0K1Y7H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "KAjFXKPDbzQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load GPT-2 tokenizer + model\n",
        "model_name = \"gpt2-medium\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kqZIVOHtZE7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset for LM"
      ],
      "metadata": {
        "id": "tWLvBzvSvevC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": \"subset_emails.txt\"})\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "tELDcryVkWqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data collator"
      ],
      "metadata": {
        "id": "4LF4nHAJviyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
      ],
      "metadata": {
        "id": "2yX01QSokaJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "1WO2LyJYvllx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gen_model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Gjj_NJXIkdsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Generate synthetic spam/ham"
      ],
      "metadata": {
        "id": "gkp48r67aTot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic spam & ham emails after fine-tuning\n",
        "\n",
        "def generate_emails(prompt, n=5, max_length=80):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=n,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
        "\n",
        "# Generate synthetic samples\n",
        "synthetic_spam = generate_emails(\"<SPAM>\", n=50)\n",
        "synthetic_ham  = generate_emails(\"<HAM>\", n=50)\n",
        "\n",
        "print(\"Example synthetic spam:\\n\", synthetic_spam[:3])\n",
        "print(\"\\nExample synthetic ham:\\n\", synthetic_ham[:3])\n",
        "\n",
        "# Build augmented dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load your real dataset again\n",
        "real_df = pd.concat([spam, ham], ignore_index=True)\n",
        "\n",
        "# Synthetic dataframe\n",
        "synthetic_df = pd.DataFrame({\n",
        "    \"text\": synthetic_spam + synthetic_ham,\n",
        "    \"label\": [\"spam\"]*len(synthetic_spam) + [\"ham\"]*len(synthetic_ham)\n",
        "})\n",
        "\n",
        "# Final augmented dataset\n",
        "augmented_df = pd.concat([real_df, synthetic_df], ignore_index=True)\n",
        "augmented_df.to_csv(\"augmented_emails.csv\", index=False)\n",
        "\n",
        "print(\" Augmented dataset saved with\", len(augmented_df), \"samples\")\n"
      ],
      "metadata": {
        "id": "fs1VVGvhsV_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.Preprocessing**"
      ],
      "metadata": {
        "id": "QOTSrHnPvrQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------------\n",
        "# Function to clean text\n",
        "# -------------------------------\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"<SPAM>|<HAM>\", \"\", text)       # remove special tokens\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)     # remove URLs\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)            # remove email addresses\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)   # keep letters + numbers\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()      # normalize spaces\n",
        "    return text.lower()\n",
        "\n",
        "# -------------------------------\n",
        "# Load augmented dataset\n",
        "# -------------------------------\n",
        "try:\n",
        "    augmented_df = pd.read_csv(\"augmented_emails.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'augmented_emails.csv' not found. Please run synthetic generation first.\")\n",
        "    exit()\n",
        "\n",
        "# -------------------------------\n",
        "# Apply cleaning to text\n",
        "# -------------------------------\n",
        "augmented_df['cleaned_text'] = augmented_df['text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Keep only cleaned text and label\n",
        "final_cleaned_augmented_df = augmented_df[['cleaned_text', 'label']]\n",
        "\n",
        "# -------------------------------\n",
        "# Save final cleaned augmented dataset\n",
        "# -------------------------------\n",
        "final_cleaned_augmented_df.to_csv(\"augmented_emails_cleaned.csv\", index=False)\n",
        "print(\" Final cleaned augmented dataset saved as 'augmented_emails_cleaned.csv' with\", len(final_cleaned_augmented_df), \"samples\")\n"
      ],
      "metadata": {
        "id": "iGO2NLRQLTeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load cleaned augmented dataset"
      ],
      "metadata": {
        "id": "09cZz5ccwRP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the final cleaned augmented dataset\n",
        "try:\n",
        "    df = pd.read_csv(\"augmented_emails_cleaned.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'augmented_emails_cleaned.csv' not found. Please run the previous steps first.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Download the cleaned augmented dataset\n",
        "files.download(\"augmented_emails_cleaned.csv\")\n"
      ],
      "metadata": {
        "id": "U8sLWxY_tViO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick overview"
      ],
      "metadata": {
        "id": "lt0sf3epOZEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\" Dataset loaded successfully\")\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df['label'].value_counts())\n"
      ],
      "metadata": {
        "id": "jPeAasAwOXVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display first 5 rows"
      ],
      "metadata": {
        "id": "LT4ri79xOcaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ubiRTwkoOart"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick overview"
      ],
      "metadata": {
        "id": "siipo4gwulSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# text length distribution"
      ],
      "metadata": {
        "id": "2qDk2sIavAaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(augmented_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "2oZEuxPbuoOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text length analysis"
      ],
      "metadata": {
        "id": "H5QCRG0Du4Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "augmented_df['text_length'] = augmented_df['cleaned_text'].astype(str).fillna('').apply(len)\n",
        "print(\"\\nText length stats:\")\n",
        "print(augmented_df['text_length'].describe())\n"
      ],
      "metadata": {
        "id": "m9R2nDOtu1ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Most common words (quick bag-of-words check)"
      ],
      "metadata": {
        "id": "1maNXBGsvTcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure the column is string type and fill NaNs before joining\n",
        "all_words = \" \".join(augmented_df['cleaned_text'].astype(str).fillna('')).split()\n",
        "word_freq = Counter(all_words)\n",
        "print(\"\\nTop 20 most common words:\")\n",
        "print(word_freq.most_common(20))"
      ],
      "metadata": {
        "id": "sRYq2RbBvGbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "KSQ58qpbF2hF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Class distribution"
      ],
      "metadata": {
        "id": "2UGgBqtluptn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x='label', palette='pastel')\n",
        "plt.title(\"Spam vs Ham Distribution\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7HQBe4v2ut6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Email Length Analysis"
      ],
      "metadata": {
        "id": "gr0ENdR74Lau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_words'] = df['cleaned_text'].astype(str).apply(lambda x: len(x.split()))\n",
        "df['num_chars'] = df['cleaned_text'].astype(str).apply(len)\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.histplot(data=df, x='num_words', hue='label', bins=50, kde=True, palette='Set2')\n",
        "plt.title(\"Distribution of Email Length (Words) by Label\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.histplot(data=df, x='num_chars', hue='label', bins=50, kde=True, palette='Set1')\n",
        "plt.title(\"Distribution of Email Length (Characters) by Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yuugAcVx4NMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot class balance"
      ],
      "metadata": {
        "id": "eCyXlUriuy4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.hist(augmented_df['text_length'], bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Email Text Length Distribution\")\n",
        "plt.xlabel(\"Length (characters)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ahoOvKnIu9pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word cloud for spam vs ham"
      ],
      "metadata": {
        "id": "EDZKLcMAubET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Separate spam and ham\n",
        "spam_text = \" \".join(augmented_df[augmented_df['label']==\"spam\"]['cleaned_text'])\n",
        "ham_text  = \" \".join(augmented_df[augmented_df['label']==\"ham\"]['cleaned_text'])\n",
        "\n",
        "# Generate word clouds\n",
        "spam_wc = WordCloud(width=800, height=400, background_color=\"white\", max_words=200).generate(spam_text)\n",
        "ham_wc  = WordCloud(width=800, height=400, background_color=\"white\", max_words=200).generate(ham_text)\n",
        "\n",
        "# Plot side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "axes[0].imshow(spam_wc, interpolation=\"bilinear\")\n",
        "axes[0].set_title(\"Spam Word Cloud\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "axes[1].imshow(ham_wc, interpolation=\"bilinear\")\n",
        "axes[1].set_title(\"Ham Word Cloud\")\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "02PoJDCmtxWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation / Co-occurrence Heatmap"
      ],
      "metadata": {
        "id": "aaHSo8EQ4ksH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Re-defining tfidf and X_tfidf from previous steps\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,   # limit vocabulary size (tune if needed)\n",
        "    ngram_range=(1,2),   # unigrams + bigrams\n",
        "    stop_words='english' # remove common stopwords\n",
        ")\n",
        "# Assuming 'df' with 'cleaned_text' is available from previous steps\n",
        "# If not, you might need to load it or ensure the previous cell that loads it is run\n",
        "try:\n",
        "    # Attempt to use the existing 'df' and 'X' if they are in the environment\n",
        "    X = df['cleaned_text'].astype(str).fillna('missingtext').apply(lambda x: x if x.strip() != '' else 'missingtext')\n",
        "    tfidf.fit(X)\n",
        "    X_tfidf = tfidf.transform(X)\n",
        "except NameError:\n",
        "    print(\"Error: 'df' or 'X' not found. Please ensure the dataset is loaded and preprocessed.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "tfidf_matrix = X_tfidf.toarray()\n",
        "\n",
        "\n",
        "top_features = np.array(feature_names)[np.argsort(np.mean(tfidf_matrix, axis=0))[-30:]]\n",
        "df_top = pd.DataFrame(tfidf_matrix[:, [np.where(feature_names==f)[0][0] for f in top_features]], columns=top_features)\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(df_top.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Between Top TF-IDF Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4_myd_wZ4nyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF vectorization"
      ],
      "metadata": {
        "id": "IyBfyOxjwiyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "metadata": {
        "id": "8s0k8rcCuOQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load cleaned augmented dataset"
      ],
      "metadata": {
        "id": "MwWFpLkB2lbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"augmented_emails_cleaned.csv\")\n",
        "\n",
        "# Drop rows with missing labels\n",
        "df.dropna(subset=['label'], inplace=True)\n"
      ],
      "metadata": {
        "id": "ZXLnGoiX2i5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare features (X) and labels (y)"
      ],
      "metadata": {
        "id": "dTBBzpcc2Nis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "X = df['cleaned_text'].astype(str)\n",
        "y = df['label']\n",
        "\n",
        "# Replace NaN or empty strings in text\n",
        "X = X.fillna('missingtext')  # Fill NaN\n",
        "X = X.apply(lambda x: x if x.strip() != '' else 'missingtext')  # Replace empty strings"
      ],
      "metadata": {
        "id": "Mf19HKsE2JVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. TF-IDF Vectorization"
      ],
      "metadata": {
        "id": "czw_667O2Qxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,   # limit vocabulary size (tune if needed)\n",
        "    ngram_range=(1,2),   # unigrams + bigrams\n",
        "    stop_words='english' # remove common stopwords\n",
        ")\n",
        "\n",
        "X_tfidf = tfidf.fit_transform(X)\n",
        "print(\" TF-IDF matrix shape:\", X_tfidf.shape)"
      ],
      "metadata": {
        "id": "z8riA9kX2RL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train-test split"
      ],
      "metadata": {
        "id": "vredaXRp2gKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", X_train.shape, \" Test set size:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "bjlbDobl2dY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Feature Insights"
      ],
      "metadata": {
        "id": "4LEzASfn4d0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Average TF-IDF scores per class\n",
        "tfidf_matrix = X_tfidf.toarray()\n",
        "labels = df['label'].values\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "spam_tfidf = tfidf_matrix[labels=='spam'].mean(axis=0)\n",
        "ham_tfidf = tfidf_matrix[labels=='ham'].mean(axis=0)\n",
        "\n",
        "top_spam_idx = np.argsort(spam_tfidf)[-20:]\n",
        "top_ham_idx = np.argsort(ham_tfidf)[-20:]\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=spam_tfidf[top_spam_idx], y=np.array(feature_names)[top_spam_idx], palette='Reds_r')\n",
        "plt.title(\"Top 20 TF-IDF Features in Spam Emails\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=ham_tfidf[top_ham_idx], y=np.array(feature_names)[top_ham_idx], palette='Blues_r')\n",
        "plt.title(\"Top 20 TF-IDF Features in Ham Emails\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YyJhYUvd4ftz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE and PCA Visualization"
      ],
      "metadata": {
        "id": "I7HVr6EB5Ogx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "subset_size = 5000\n",
        "if X_tfidf.shape[0] > subset_size:\n",
        "    X_sample = X_tfidf[:subset_size].toarray()\n",
        "    y_sample = labels[:subset_size]\n",
        "else:\n",
        "    X_sample = X_tfidf.toarray()\n",
        "    y_sample = labels\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pxgEuiuL44lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  PCA for 2D visualization"
      ],
      "metadata": {
        "id": "U47HmXqA5EiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_sample)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y_sample, palette={'spam':'red','ham':'blue'}, alpha=0.6)\n",
        "plt.title(\"PCA Projection of Emails (2D)\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q2zw78t95H6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  t-SNE for 2D visualization"
      ],
      "metadata": {
        "id": "2r65J9_b5BSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=40, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(X_sample)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y_sample, palette={'spam':'red','ham':'blue'}, alpha=0.6)\n",
        "plt.title(\"t-SNE Projection of Emails (2D)\")\n",
        "plt.xlabel(\"t-SNE Component 1\")\n",
        "plt.ylabel(\"t-SNE Component 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tvt_bSw947-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **statistical tests**"
      ],
      "metadata": {
        "id": "K6hgN-8i259T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## statistical tests to compare real vs synthetic distributions."
      ],
      "metadata": {
        "id": "q-2YthsnznXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "import pandas as pd\n",
        "from collections import Counter # Import Counter\n",
        "\n",
        "# Load spam + ham (already saved earlier in cell fs1VVGvhsV_5)\n",
        "spam_file = \"/content/drive/MyDrive/spam.txt\"\n",
        "ham_file = \"/content/drive/MyDrive/ham.txt\"\n",
        "\n",
        "# Read into pandas - ensures column is named 'text'\n",
        "spam = pd.read_csv(spam_file, names=[\"text\"], keep_default_na=False) # Added keep_default_na=False\n",
        "ham = pd.read_csv(ham_file, names=[\"text\"], keep_default_na=False)   # Added keep_default_na=False\n",
        "\n",
        "real_df = pd.concat([spam, ham], ignore_index=True)\n",
        "# Rename the 'text' column to 'cleaned_text' to match the rest of the notebook\n",
        "real_df.rename(columns={'text': 'cleaned_text'}, inplace=True)\n",
        "try:\n",
        "\n",
        "    synthetic_df = pd.DataFrame({\n",
        "        \"text\": synthetic_spam + synthetic_ham,\n",
        "        \"label\": [\"spam\"]*len(synthetic_spam) + [\"ham\"]*len(synthetic_ham)\n",
        "    })\n",
        "    # Rename the 'text' column to 'cleaned_text'\n",
        "    synthetic_df.rename(columns={'text': 'cleaned_text'}, inplace=True)\n",
        "\n",
        "except NameError:\n",
        "    print(\"synthetic_spam or synthetic_ham not found. Please generate synthetic data first.\")\n",
        "    # Fallback: Try loading from augmented_emails.csv and filtering\n",
        "    try:\n",
        "        augmented_df_full = pd.read_csv(\"augmented_emails.csv\")\n",
        "        synthetic_df = augmented_df_full[augmented_df_full['label'].isin(['spam', 'ham'])].copy() # Filter for synthetic labels\n",
        "        if 'text' in synthetic_df.columns and 'cleaned_text' not in synthetic_df.columns:\n",
        "             synthetic_df.rename(columns={'text': 'cleaned_text'}, inplace=True)\n",
        "        # Ensure 'cleaned_text' exists\n",
        "        if 'cleaned_text' not in synthetic_df.columns:\n",
        "             print(\"Could not find 'text' or 'cleaned_text' in augmented_emails.csv for synthetic data.\")\n",
        "             exit() # Exit if data is not found\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"augmented_emails.csv not found. Cannot load synthetic data.\")\n",
        "        exit()\n",
        "if 'cleaned_text' not in real_df.columns or 'cleaned_text' not in synthetic_df.columns:\n",
        "    print(\"Error: 'cleaned_text' column not found in one of the dataframes.\")\n",
        "    exit()\n",
        "real_text = real_df['cleaned_text'].astype(str).fillna('') # Ensure string type and fill NaNs\n",
        "synthetic_text = synthetic_df['cleaned_text'].astype(str).fillna('') # Ensure string type and fill NaNs\n",
        "\n",
        "# Build word frequency tables\n",
        "def get_top_words(texts, n=50):\n",
        "    # Filter out empty strings before joining\n",
        "    all_words = \" \".join([text for text in texts if text]).split()\n",
        "    freq = Counter(all_words)\n",
        "    return dict(freq.most_common(n))\n",
        "\n",
        "real_top = get_top_words(real_text, 50)\n",
        "synthetic_top = get_top_words(synthetic_text, 50)\n",
        "\n",
        "# Merge into dataframe\n",
        "words = list(set(real_top.keys()) | set(synthetic_top.keys()))\n",
        "freq_df = pd.DataFrame({\n",
        "    \"word\": words,\n",
        "    \"real\": [real_top.get(w,0) for w in words],\n",
        "    \"synthetic\": [synthetic_top.get(w,0) for w in words]\n",
        "})\n",
        "contingency_table = freq_df[['real', 'synthetic']].values\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-ElNh1GzfPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Chi-square test"
      ],
      "metadata": {
        "id": "eiAkGvs33vmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# chi2_contingency expects observed frequencies in a 2D array\n",
        "if contingency_table.shape[0] > 1 and contingency_table.shape[1] == 2: # Need at least 2 rows for Chi-squared\n",
        "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "    print(\"Chi-Square Test Statistic:\", chi2)\n",
        "    print(\"p-value:\", p)\n",
        "\n",
        "    # Interpretation\n",
        "    alpha = 0.05\n",
        "    print(\"\\nInterpretation:\")\n",
        "    if p < alpha:\n",
        "        print(f\"The p-value ({p:.4f}) is less than the significance level ({alpha}). We reject the null hypothesis.\")\n",
        "        print(\"There is a statistically significant difference between the word frequency distributions of the real and synthetic datasets.\")\n",
        "    else:\n",
        "        print(f\"The p-value ({p:.4f}) is greater than the significance level ({alpha}). We fail to reject the null hypothesis.\")\n",
        "        print(\"There is no statistically significant difference between the word frequency distributions of the real and synthetic datasets.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Chi-square test: Not enough data (less than 2 words) to perform the test.\")\n",
        "\n",
        "print(\"\\nWord Frequencies:\")\n",
        "display(freq_df.sort_values(by='real', ascending=False).head())"
      ],
      "metadata": {
        "id": "y-H6Rpd23t9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection (Initial Research)\n",
        "\n",
        "Based on the EDA, dataset characteristics, and augmented TF-IDF features, we shortlist the following models for initial research and experimentation:\n",
        "\n",
        "## 1️⃣ Supervised Learning (Labeled Data: Spam vs Ham)\n",
        "- **Naive Bayes (MultinomialNB / BernoulliNB)**: Fast, strong baseline for text classification, works well with TF-IDF.  \n",
        "- **Logistic Regression**: Linear model suitable for high-dimensional TF-IDF vectors; interpretable and probabilistic outputs.  \n",
        "- **Linear SVM**: Effective for high-dimensional sparse data; robust classifier for text.  \n",
        "- **Simple Feedforward Neural Network (MLP)**: Optional deep learning model to explore non-linear relationships in TF-IDF features.  \n",
        "- **Transformer (DistilBERT / BERT)**: Optional state-of-the-art model capturing context and semantics; requires GPU.\n",
        "\n",
        "## 2️⃣ Unsupervised Learning (No Labels / Clustering / Anomaly Detection)\n",
        "- **K-Means Clustering**: Group emails into “spam-like” vs “ham-like” clusters; simple and fast.  \n",
        "- **DBSCAN**: Detect clusters and outliers (rare spam) in feature space; handles irregular patterns.  \n",
        "- **Isolation Forest**: Detect rare or anomalous spam emails; works well for imbalanced data.  \n",
        "- **One-Class SVM**: Learn “normal” ham emails and detect spam as anomalies; suitable for high-dimensional TF-IDF features.\n",
        "\n",
        "## 3️⃣ Deep Learning / Neural Networks\n",
        "- **MLP (Feedforward NN)**: Flexible for TF-IDF features; can model non-linear relationships.  \n",
        "- **CNN for Text**: Good for local n-gram patterns if using word embeddings.  \n",
        "- **RNN / LSTM / GRU**: Capture sequential patterns in emails; useful for context-aware modeling.  \n",
        "- **Transformer (BERT / DistilBERT / RoBERTa)**: State-of-the-art NLP model; handles semantic and contextual information.  \n",
        "- **Hybrid Models (CNN + LSTM / Attention)**: Powerful but complex; optional for advanced experiments.\n",
        "\n",
        "## Notes\n",
        "- **Start with supervised models** (Naive Bayes, Logistic Regression, Linear SVM) as quick baselines.  \n",
        "- **Unsupervised methods** can be used to explore **anomaly detection** or identify clusters of spam for additional insights.  \n",
        "- **Deep learning models** are optional and recommended if GPU is available or you need higher accuracy.  \n",
        "- Final model selection will depend on **training results, evaluation metrics, and computational efficiency**.\n"
      ],
      "metadata": {
        "id": "IN5HlZFo8NHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: Data Augmentation & Model Preparation"
      ],
      "metadata": {
        "id": "k9OLDgpL8no0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**1. Data Inspection and Cleaning**  \n",
        "- Loaded and inspected the original email dataset.  \n",
        "- Split emails into **spam and ham** categories.  \n",
        "- Applied **text cleaning** to remove special characters, URLs, email addresses, and normalize text.\n",
        "\n",
        "**2. Synthetic Data Generation**  \n",
        "- Fine-tuned a **GPT-2 model** on real email data to learn spam and ham styles.  \n",
        "- Generated **high-quality synthetic emails** to address class imbalance.  \n",
        "- Preprocessed synthetic data to match the real email format.\n",
        "\n",
        "**3. Dataset Augmentation**  \n",
        "- Combined real and synthetic emails to create the **final augmented dataset**.  \n",
        "- Saved the cleaned dataset as `augmented_emails_cleaned.csv` for future use.  \n",
        "- Ensured it is **ready for machine learning workflows**.\n",
        "\n",
        "**4. Exploratory Data Analysis (EDA)**  \n",
        "- Quick overview: dataset size, missing values, class distribution.  \n",
        "- Advanced visualizations:  \n",
        "  - Spam vs ham distribution, email length histograms  \n",
        "  - Word clouds for frequent words  \n",
        "  - Top TF-IDF features per class  \n",
        "  - Correlation heatmap of TF-IDF features  \n",
        "  - PCA and t-SNE projections to visualize class separation\n",
        "\n",
        "**5. Feature Engineering**  \n",
        "- Applied **TF-IDF vectorization** to transform text into numerical features.  \n",
        "- Limited vocabulary to **5000 features** (unigrams + bigrams) and removed common stopwords.\n",
        "\n",
        "**6. Statistical Testing**  \n",
        "- Chi-Square test showed **statistically significant differences** between real and synthetic word distributions.  \n",
        "- Indicates synthetic data differs in word frequency but still adds value for augmentation.\n",
        "\n",
        "**7. Model Selection (Initial Research)**  \n",
        "- **Supervised models**: Naive Bayes, Logistic Regression, Linear SVM (recommended baselines).  \n",
        "- **Optional deep learning**: MLP, CNN, RNN/LSTM, Transformers.  \n",
        "- **Unsupervised / anomaly detection**: K-Means, DBSCAN, Isolation Forest, One-Class SVM.  \n",
        "- Models balance **speed, interpretability, and accuracy potential**.\n",
        "\n",
        "**8. Next Steps**  \n",
        "- Initialize selected models and perform **training and evaluation**.  \n",
        "- Compare performance using **Accuracy, Precision, Recall, and F1-Score**.  \n",
        "- Explore optional **deep learning or unsupervised methods** for improvements.  \n",
        "- Dataset `augmented_emails_cleaned.csv` is ready for **reuse**.\n",
        "\n",
        "**Overall Summary:**  \n",
        "- Created a **robust, balanced, and clean dataset**.  \n",
        "- Conducted **advanced EDA** to understand patterns and features.  \n",
        "- Provided a **structured model selection roadmap**.  \n",
        "- All artifacts are **reusable and reproducible**, enabling efficient spam detection experiments.\n"
      ],
      "metadata": {
        "id": "NunB2w6S85bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wzqn0k_YPPPC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}