{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating the synthetic data**"
      ],
      "metadata": {
        "id": "uvBnqH2JJu2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount drive"
      ],
      "metadata": {
        "id": "m3DcCmDhJoeI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky7oVPi8R0E9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH2r2hhLSl23"
      },
      "source": [
        "# Load your fine-tuned GPT-2 model & tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iOCE92DSlbQ"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/finetuned_gpt2_emails\"  # path to the folder in Drive\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "model.to(\"cpu\")  # use CPU if GPU is not available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPqjmfNvSrQ3"
      },
      "source": [
        "# Generate synthetic emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ber9FzaaStUl"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def generate_unique_emails(prompt, target_count=1000, max_length=80, batch_size=20):\n",
        "    generated_set = set()\n",
        "\n",
        "    while len(generated_set) < target_count:\n",
        "        n = min(batch_size, target_count - len(generated_set))\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "        outputs = model.generate(\n",
        "            inputs.repeat(n, 1),\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=n,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        for o in outputs:\n",
        "            email = tokenizer.decode(o, skip_special_tokens=True).strip()\n",
        "            if email and email not in generated_set:\n",
        "                generated_set.add(email)\n",
        "\n",
        "    return list(generated_set)\n",
        "\n",
        "#  Generate 3500 spam and 3500 ham\n",
        "synthetic_spam = generate_unique_emails(\"<SPAM>\", target_count=3500)\n",
        "synthetic_ham = generate_unique_emails(\"<HAM>\", target_count=3500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmNqW0GGreax"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your real dataset\n",
        "real_df = pd.read_csv(\"/content/drive/MyDrive/CleanedDataset.csv\", low_memory=False)\n",
        "\n",
        "print(\" Real dataset loaded with:\", real_df.shape)\n",
        "\n",
        "# Build synthetic dataframe\n",
        "synthetic_df = pd.DataFrame({\n",
        "    \"text\": synthetic_spam + synthetic_ham,\n",
        "    \"label\": [\"spam\"] * len(synthetic_spam) + [\"ham\"] * len(synthetic_ham)\n",
        "})\n",
        "\n",
        "print(\" Synthetic dataset created with:\", synthetic_df.shape)\n",
        "\n",
        "# Combine real + synthetic\n",
        "augmented_df = pd.concat([real_df, synthetic_df], ignore_index=True)\n",
        "\n",
        "# Drop duplicates (just in case)\n",
        "augmented_df.drop_duplicates(subset=['text'], inplace=True)\n",
        "\n",
        "# Save final augmented dataset\n",
        "augmented_df.to_csv(\"/content/drive/MyDrive/augmented_emails.csv\", index=False)\n",
        "\n",
        "print(f\" Augmented dataset ready with {len(augmented_df)} samples and saved as 'augmented_emails.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwylhkqzsY2j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}